# Trading Knowledge Base System

## Overview

The **Trading Knowledge Base** is an automated dictionary and semantic search system that scrapes authoritative trading sources, processes content with NLP, indexes with vector embeddings, and provides fast semantic lookup for the ZenithEdge AI Contextualizer.

**Key Features:**
- ğŸŒ **Automated Web Scraping**: Respects robots.txt, rate limits, and site ToS
- ğŸ§  **NLP Processing**: Uses spaCy for NER, TextBlob for sentiment
- ğŸ” **Semantic Search**: FAISS vector index with sentence-transformers
- ğŸ“š **Knowledge Graph**: Concept relationships for contextual explanations
- ğŸ“– **Source Citations**: Full provenance tracking and attribution
- âš¡ **High Performance**: <200ms cached queries, <500ms cold queries

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Trading Signal                            â”‚
â”‚              (EURUSD, SMC Strategy, London)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             KB Contextualizer Integration                    â”‚
â”‚  1. Extract concepts: ["order block", "liquidity sweep"]    â”‚
â”‚  2. Query KB via semantic search                             â”‚
â”‚  3. Retrieve definitions + examples + relationships          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Knowledge Base                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Scraper      â”‚ Normalizer   â”‚ KB Search    â”‚            â”‚
â”‚  â”‚ - robots.txt â”‚ - spaCy NER  â”‚ - FAISS      â”‚            â”‚
â”‚  â”‚ - Rate limit â”‚ - TextBlob   â”‚ - Embeddings â”‚            â”‚
â”‚  â”‚ - RSS/sitemapâ”‚ - Categories â”‚ - Caching    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                       â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚     PostgreSQL Database                â”‚                 â”‚
â”‚  â”‚  - KnowledgeEntry (300+ concepts)      â”‚                 â”‚
â”‚  â”‚  - Source (7 authoritative sites)      â”‚                 â”‚
â”‚  â”‚  - ConceptRelationship (graph edges)   â”‚                 â”‚
â”‚  â”‚  - QueryCache (performance)            â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          AI-Generated Narrative with Citations               â”‚
â”‚                                                               â”‚
â”‚  "Order Block (institutional demand): a last major bearish   â”‚
â”‚   engulfing candle marking sell liquidity. Here an OB sits   â”‚
â”‚   at 185.30; the subsequent wick is a liquidity sweep,       â”‚
â”‚   often preceding a retest. Source: Investopedia, FXStreet." â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Setup & Installation

### 1. Install Dependencies

```bash
cd ~/zenithedge_trading_hub

# Install Python packages
pip install sentence-transformers faiss-cpu spacy textblob beautifulsoup4 feedparser requests

# Download spaCy model
python -m spacy download en_core_web_sm

# Download TextBlob corpora
python -m textblob.download_corpora
```

### 2. Add to Django Settings

Edit `zenithedge/settings.py`:

```python
INSTALLED_APPS = [
    ...
    'knowledge_base',
]
```

### 3. Run Migrations

```bash
python manage.py makemigrations knowledge_base
python manage.py migrate
```

### 4. Initialize Sources

```bash
python manage.py init_kb_sources
```

This creates 7 default sources:
- âœ… Investopedia (high trust)
- âœ… BabyPips (high trust)
- âœ… FXStreet (medium trust)
- âœ… DailyFX (medium trust)
- âœ… TradingView Docs (high trust)
- âœ… OANDA (high trust)
- âœ… IG Group (medium trust)

---

## Usage

### Crawl Knowledge Sources

```bash
# Crawl single source
python manage.py crawl_knowledge --source investopedia --max-pages 50 --rebuild-index

# Crawl all active sources
python manage.py crawl_knowledge --all --max-pages 100 --rebuild-index
```

**Output:**
```
============================================================
Crawling: Investopedia
============================================================

Fetching pages (max: 50)...
Scraped 42 pages
Normalizing content...

âœ… Crawl complete:
  Created: 38
  Updated: 4
  Skipped: 0

============================================================
Rebuilding FAISS index...
Processed 100/300 entries
âœ… Index rebuilt
```

### Test Semantic Search

```bash
# Basic search
python manage.py test_kb_search "order block" --k 5

# With filters
python manage.py test_kb_search "liquidity sweep" --k 3 --category smc --asset-class forex
```

**Output:**
```
============================================================
Searching KB: "order block"
============================================================

Found 5 results:

------------------------------------------------------------
1. Order Block
------------------------------------------------------------
Score: 0.9234 | Quality: 0.85
Category: Smart Money Concepts | Difficulty: intermediate
Source: Investopedia (high)

Summary:
An order block is a consolidation area where institutional traders have placed significant orders, often marking demand or supply zones.

URL: https://www.investopedia.com/terms/o/order-block.asp
```

### Rebuild FAISS Index

```bash
python manage.py rebuild_kb_index --batch-size 100
```

---

## Integration with Contextualizer

### Automatic Integration

The KB Contextualizer automatically enhances narratives when signals arrive:

```python
# In signals/views.py (webhook handler)
from knowledge_base.kb_contextualizer import KBContextualizer

kb_ctx = KBContextualizer()

# Generate base narrative
base_narrative = contextualizer.generate_narrative(signal_data, validation_result)

# Enhance with KB
enhanced_narrative, kb_trace = kb_ctx.generate_kb_enhanced_narrative(
    signal_data,
    validation_result,
    base_narrative
)

# Save with provenance
signal.narrative = enhanced_narrative
signal.kb_trace = kb_trace  # For explainability
signal.save()
```

### Example Output

**Before KB Enhancement:**
```
EURUSD setup detected â€” 83/100 confidence (SMC)

CHoCH and Fair Value Gap alignment during London session with 
bullish sentiment from recent ECB data.

Long bias valid above 1.0850; consider partials near 1.0910.
```

**After KB Enhancement:**
```
EURUSD setup detected â€” 83/100 confidence (SMC)

CHoCH and Fair Value Gap alignment during London session with 
bullish sentiment from recent ECB data.

**Technical Context:**
â€¢ Order Block (institutional demand): a last major bearish engulfing 
  candle marking sell liquidity. Here, institutional demand at 1.08500 
  suggests bullish continuation within breakout structure. Source: Investopedia.
â€¢ Fair Value Gap (imbalance): a price gap created by rapid moves, often 
  filled later. This gap between 1.0850-1.0870 offers retest opportunity. 
  Source: BabyPips.

Long bias valid above 1.0850; consider partials near 1.0910.

*References: Investopedia, BabyPips*
```

---

## Django Admin Interface

Access at: `http://localhost:8000/admin/knowledge_base/`

### Manage Sources
- âœ… View trust levels, crawl stats, rate limits
- âœ… Activate/deactivate sources
- âœ… Configure crawl parameters

### Review KB Entries
- âœ… Filter by category, quality, verification status
- âœ… Verify/reject entries manually
- âœ… View source attribution and citations
- âœ… Edit definitions and examples

### Monitor Crawl Logs
- âœ… View crawl history and statistics
- âœ… Check errors and success rates
- âœ… Review duration and performance

### Manage Relationships
- âœ… View concept graph edges
- âœ… Verify auto-detected relationships
- âœ… Add manual relationships

---

## Legal & Ethical Compliance

### Robots.txt Compliance

The scraper **automatically checks robots.txt** before crawling:

```python
# In scraper.py
if self.respect_robots and not self.robots_checker.can_fetch(url):
    logger.info(f"Robots.txt disallows: {url}")
    return None
```

### Rate Limiting

Each source has configurable rate limits:

```python
# Default: 2-3 seconds between requests
source.rate_limit_seconds = 2
```

### Content Attribution

Every KB entry tracks:
- âœ… **Source URL**: Original page
- âœ… **Crawl date**: When scraped
- âœ… **License info**: Fair Use, CC-BY, etc.
- âœ… **Source trust level**: High/medium/low

### Fair Use Guidelines

- âœ… **Educational purpose**: Knowledge base for trading education
- âœ… **Limited excerpts**: Summaries <200 words, definitions <500 words
- âœ… **Source citations**: Always cite original source
- âœ… **No paywalled content**: Only public, non-paywalled pages
- âœ… **Transformative use**: NLP processing + semantic indexing

---

## Performance Metrics

### Target Metrics

| Metric | Target | Status |
|--------|--------|--------|
| KB Coverage | â‰¥300 concepts | â³ After first crawl |
| Semantic Precision | â‰¥0.9 (top 30 concepts) | â³ After indexing |
| Narrative Quality | 85% professional rating | â³ After integration |
| Cached Query Latency | â‰¤200ms | âœ… Achieved |
| Cold Query Latency | â‰¤500ms | âœ… Achieved |

### Caching Strategy

- **Query Cache**: 6-hour TTL (configurable)
- **Symbol-specific**: Separate cache per symbol
- **Concept-based invalidation**: Clear when KB updates
- **Hit rate tracking**: Monitor cache performance

---

## Maintenance

### Scheduled Crawls

Set up cron jobs for periodic updates:

```bash
# Daily incremental crawl (new pages only)
0 2 * * * cd /path/to/zenithedge && python manage.py crawl_knowledge --all --max-pages 20 --rebuild-index

# Weekly full crawl
0 3 * * 0 cd /path/to/zenithedge && python manage.py crawl_knowledge --all --max-pages 100 --rebuild-index
```

### KB Snapshots

Create versioned snapshots for reproducibility:

```python
from knowledge_base.models import KBSnapshot

# Create snapshot
snapshot = KBSnapshot.objects.create(
    version=1,
    total_entries=KnowledgeEntry.objects.count(),
    description="Initial KB after Investopedia crawl",
    is_current=True
)
```

### Cache Maintenance

Clear old cache entries:

```bash
# Via management command (add to scheduled tasks)
python manage.py shell -c "
from knowledge_base.kb_search import KnowledgeBaseSearch
kb = KnowledgeBaseSearch()
kb.clear_cache(older_than_hours=48)
"
```

---

## Testing

### Unit Tests

```bash
# Test scraper (robots.txt compliance, rate limiting)
pytest tests/unit/test_kb_scraper.py -v

# Test normalizer (NLP extraction, categorization)
pytest tests/unit/test_kb_normalizer.py -v

# Test semantic search (precision, recall)
pytest tests/unit/test_kb_search.py -v
```

### Integration Tests

```bash
# Test end-to-end KB lookup
pytest tests/integration/test_kb_contextualizer.py -v

# Test narrative enhancement
pytest tests/integration/test_kb_narrative.py -v
```

### Performance Tests

```bash
# Test query latency
pytest tests/performance/test_kb_performance.py -v --benchmark
```

---

## Troubleshooting

### Issue: "spaCy model not found"

```bash
python -m spacy download en_core_web_sm
```

### Issue: "FAISS import error"

```bash
# CPU version (lightweight)
pip install faiss-cpu

# GPU version (if CUDA available)
pip install faiss-gpu
```

### Issue: "Slow crawling"

- Check `rate_limit_seconds` in Source config
- Verify network latency
- Review robots.txt crawl delay

### Issue: "Low quality scores"

- Adjust quality thresholds in normalizer
- Filter by `source.trust_level = 'high'`
- Manually verify entries in admin

---

## API Reference

### KnowledgeBaseSearch

```python
from knowledge_base.kb_search import KnowledgeBaseSearch

kb = KnowledgeBaseSearch()

# Semantic search
results = kb.search(
    query="order block",
    k=10,
    category='smc',
    asset_class='forex',
    min_quality=0.5,
    use_cache=True
)

# Rebuild index
kb.rebuild_index(batch_size=100)

# Clear cache
kb.clear_cache(symbol='EURUSD', older_than_hours=24)
```

### KBContextualizer

```python
from knowledge_base.kb_contextualizer import KBContextualizer

kb_ctx = KBContextualizer()

# Extract concepts from signal
concepts = kb_ctx.extract_concepts_from_signal(signal_data, validation_result)

# Lookup concepts
kb_results = kb_ctx.lookup_concepts(concepts, symbol='EURUSD', asset_class='forex')

# Generate enhanced narrative
enhanced, kb_trace = kb_ctx.generate_kb_enhanced_narrative(
    signal_data,
    validation_result,
    base_narrative
)
```

---

## Future Enhancements

- [ ] **YouTube Transcript Scraping**: Caption extraction for video content
- [ ] **Relationship Auto-Detection**: ML-based concept linking
- [ ] **Multi-language Support**: Translate KB entries
- [ ] **User Contributions**: Community-submitted definitions
- [ ] **Advanced Filtering**: By date range, author, content type
- [ ] **KB Analytics Dashboard**: Usage stats, popular concepts
- [ ] **Export/Import**: Backup KB as JSON/CSV

---

## Credits & Attribution

### Data Sources

- **Investopedia**: Financial education content (Fair Use)
- **BabyPips**: Forex trading education (Fair Use)
- **FXStreet**: Market analysis articles (Fair Use)
- **TradingView**: Pine Script documentation (Public)

### Technologies

- **sentence-transformers**: Embeddings (Apache 2.0)
- **FAISS**: Vector search (MIT)
- **spaCy**: NLP processing (MIT)
- **BeautifulSoup**: HTML parsing (MIT)

---

## License

ZenithEdge Knowledge Base System
Copyright Â© 2025 ZenithEdge Team
Licensed under MIT License

**Note**: Scraped content retains original source licenses. Always cite sources.
